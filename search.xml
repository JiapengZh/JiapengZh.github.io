<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>常用激活函数进化史</title>
    <url>/2020/02/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/</url>
    <content><![CDATA[<p>在使用神经网络时，我们要加入非线性的函数，这样神经网络学到的 “边界” 才会更加的精确、光滑，神经网络的 “黑箱” 才可以表示更多复杂的函数来解决更加复杂的问题。</p>
<ol>
<li><p>Sigmoid 激活函数。我们知道在做二分类时我们经常使用sigmoid函数，它可以表示 P(y=1|x) 的概率，但是我们基本只在输出层使用它，几乎不在神经网络中间使用它，是因为如下图，如果输入落在0附近，梯度是相当大的，我们迭代weights时一般使用Backpropagation，根据求导的链式法则，由于神经网络层数一层层叠加，就会出现梯度爆炸的问题，一般我们使用梯度裁剪即Gradient Clipping来解决梯度爆炸问题；而如果输入不落在中间区间，sigmoid函数saturate，saturated neurons kill the gradient flow，梯度逐渐趋近于0，这时候会出现梯度消失问题，对于梯度消失问题我们可以使用ReLU函数来解决。而且Sigmoid 函数输出一直是positive的，s = sigmoid(f(x))其中f(x)=wx+b, 对w求导根据链式法则我们可以得到<br>$$<br>\frac{\partial s}{\partial f} * \frac{\partial f}{\partial w} =\frac{\partial s}{\partial f}<em>x<br>$$<br>前一项大于0，而x大于零或者小于0，就使得在梯度下降迭代weights时，it is going to be either positive or negative for *</em>all** weights, 这时候梯度下降的路线会出现<strong>Zig Zag path</strong>，使得优化过程变得缓慢，原本可以一步到位的可能多出好多倍的时间。</p>
<ul>
<li>优：可以很好的表示概率P(y=1|x)</li>
<li>缺：梯度爆炸；梯度消失; Zig Zag path</li>
</ul>
<p>$$<br>f(x)=\frac{1}{1+exp(-x)}<br>$$</p>
</li>
</ol>
   <img src="http:\\JiapengZh.github.io\1\sigmoid.png" alt="Sigmoid" style="zoom:50%;">

<ol start="2">
<li>Tanh 激活函数。 类似于sigmoid的函数，在sigmoid基础上变为<strong>zero-centered</strong>，更容易被优化，但是梯度爆炸以及消失的缺点仍然存在。<br>$$<br>tanh(x)=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}<br>$$</li>
</ol>
   <img src="/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/blog_img\tanh.png" alt="Tanh" style="zoom:50%;">

<ol start="3">
<li><p>ReLU 激活函数。ReLU函数由于梯度在第一象限恒为1，所以可以很好解决梯度消失的问题，并且由于梯度为一个constant，所以使用梯度下降时间以及空间复杂度会十分低，但是由于在输入为负时，梯度会负，这时候会有死亡ReLU的问题。</p>
<ul>
<li>优：避免梯度消失；时间空间复杂度更低</li>
<li>缺：无法避免梯度爆炸；死亡ReLU</li>
</ul>
<p>$$<br>f = max(0, x)<br>$$</p>
</li>
</ol>
   <img src="/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/blog_img\ReLU.png" alt="ReLU" style="zoom:50%;">

<ol start="4">
<li><p>ELU 激活函数。在ReLU基础上让其在输入为负时梯度不为0，但是由于引入指数运算，计算复杂度会大大上升，并且α是一个hyper parameter，不能被学习到，会影响神经网络的性能。</p>
<ul>
<li>优：继承ReLU优点，避免死亡ReLU的问题</li>
<li>缺：无法避免梯度爆炸；指数运算复杂；引入新的hyper parameter</li>
</ul>
<p>$$<br>f = \begin{cases}x&amp;x&gt;0\α(exp(x)-1)&amp;x\leq0\end{cases}<br>$$</p>
</li>
</ol>
   <img src="/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/blog_img\ELU.png" alt="ELU" style="zoom:50%;">

<ol start="5">
<li><p>Leaky ReLU激活函数。相比ELU，无指数运算，大大降低计算复杂度，但是由于正负时均为线性函数，所以相比ELU，有一些局限性</p>
<ul>
<li>优：继承ELU优点；无指数运算，计算复杂度大大下降</li>
<li>缺：无法避免梯度爆炸；相比ELU，由于正负部分均为线性，有一定局限性；引入了一个新的hyper parameter</li>
</ul>
<p>$$<br>f(x) = max(αx,x)<br>$$</p>
</li>
</ol>
   <img src="/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/blog_img\Leaky ReLU.png" alt="Leaky ReLU" style="zoom:50%;">

<ol start="6">
<li><p>SELU 激活函数。收敛快，不会出现梯度爆炸或者消失问题(<strong>internal normalization</strong>), 但是对初始化有特殊要求(<strong>initialization function lecun_normal</strong>)。</p>
<p>$$<br>f = λ \begin{cases}x&amp;x&gt;0\αexp(x)-α&amp;x\leq0\end{cases}<br>$$</p>
</li>
<li><p>Maxout Units。注意是很多个Units共同组成（数目不定）。多个线性的分界线可以去逼近一个convex function。个数越多，越逼近</p>
<ul>
<li>优：不会死；不会saturate；Linear Regimes；Generalization of ReLUs</li>
<li>缺：会大大增加参数量</li>
</ul>
<p>$$<br>Maxout = max(w_1^Tx+b_1,w_2^Tx+b_2)<br>$$</p>
<img src="/11/%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%BF%9B%E5%8C%96%E5%8F%B2/blog_img\Maxout.png" alt="Maxout Units" style="zoom:50%;">



</li>
</ol>
<p><strong>一些激活函数使用建议:</strong></p>
<ul>
<li>Sigmoid 函数一般真的不用:)</li>
<li>ReLU是最普遍使用的激活函数</li>
<li>除了ReLU的第二选择是ReLU的变种（ELU，Leaky ReLU等等）或者Maxout激活函数</li>
<li>Recurrent nets需要使用tanh或者相似的函数</li>
</ul>
]]></content>
      <categories>
        <category>学科常见问题汇总</category>
      </categories>
  </entry>
  <entry>
    <title>基于双目相机计算机视觉任务难点</title>
    <url>/2020/02/10/%E5%9F%BA%E4%BA%8E%E5%8F%8C%E7%9B%AE%E7%9B%B8%E6%9C%BA%E7%9B%AE%E6%A0%87%E8%BF%BD%E8%B8%AA%E9%9A%BE%E7%82%B9/</url>
    <content><![CDATA[<ul>
<li>双目相机基线窄对近处物体成像效果好，基线宽对远处物体成像效果好，这是一个trade-off，如何使得两者平衡，是很多团队正在研究的方向，有一些团队使用了多台不同基线长度的双目相机来解决这个问题</li>
</ul>
<ul>
<li>在进行3D重构时，往往要求双目相机的光轴互相平行，这是因为在3D重构时，我们在两张双目相机得到的图片上寻找匹配点时，如果双目相机光轴平行，我们可以只考虑一个方向，我们可以在图片上沿水平方向寻找匹配点，这大大缩短了匹配算法的时间以及提升了精度；而现实生活中，由于安装等等原因，可能造成双目相机光轴不平行。当然如果相机光轴不平行时，可以通过八点法等方法计算Rotation matrix R 以及 Translation matrix T 来得到两个相机的位置变换，再进行匹配算法，3D重构等(在我的Github中有匹配算法，求相机变换矩阵RT的基于RANSAC算法的八点法以及物体3D重构的代码实现)</li>
</ul>
<ul>
<li>在3D环境中目标匹配比较难，Detector在不同的frame中探测到很多个物体，但是Data association ( Similarity measurement ) 比较难，当然这也是传统MoT(多目标追踪) 的难点，使得我们将这些物体匹配起来比较难，也就无从谈及追踪了</li>
</ul>
]]></content>
      <categories>
        <category>学科常见问题汇总</category>
      </categories>
  </entry>
</search>
